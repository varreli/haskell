<crestfallen> Hi I'm looking at the evaluation order from filterM powerset [4,5]. This is a textbook example. Online I found the applicative version with the evaluation written out. It's calling the list elements "flags," like flg1 flg2. I understand the bools create a cartesian product of the "powerset", but I don't follow precisely what is happening in the evaluation (the paste includes the handwritten evaluation) There's no step in the evaluation where flg2 
<crestfallen> (False?) is included. https://termbin.com/i9eq   thanks for any explanation
<Cale> That's a really confusing choice of name for (const [True, False])
<crestfallen> so since the applicative version uses foldr, the output would be different, but in any case I cannot follow the evaluation, or imagine what the order would look like with the Monad version
<Cale> But yeah, when you use filterM, it's going to construct the powerset
<Cale> The idea is that in the list monad, "running" a list means picking an element from it in all possible ways (and collecting a list of the eventual results)
<dsal> "evaluation order" is not something I think about much in Haskell.
<Cale> and so, when you apply the predicate for the items in your list
<Cale> it tells you "take it or leave it"
<Cale> and so you do this in all possible ways
<Cale> and collect up a list of the possible results of keeping or dropping each element
<crestfallen> thanks Cale I figured that much, but why is there no case where flg2 (False) is mentioned there?
<Cale> What's flg2?
<crestfallen> flags = [True,False], so I was assuming that False was flg2
<Cale> I see you have flg1 as a parameter to the lambda in filtMM
<Cale> I don't know why it's flg1
<Cale> Weird names
<crestfallen> so I guess I cannot follow each step from starting with [[]] 
<crestfallen> one moment pls
<crestfallen> the applicative version comes from this blog: https://blog.ssanj.net/posts/2018-04-10-how-does-filterm-work-in-haskell.html
<Cale> Let's simplify the first definition of filterMM by specialising p = (\x -> [True, False])
<Cale> So m is the list monad
<Cale> filterMM (\x -> [True, False]) [] = return [] = [[]]
<Cale> filterMM (\x -> [True, False]) (x:xs) = do
<Cale>   b <- [True, False]
<Cale>   ys <- filterMM (\x -> [True, False]) xs
<Cale>   return (if b then x:ys else ys)
<crestfallen> ok
<Cale> So, assuming that filterMM (\x -> [True, False]) xs will compute the powerset of xs
<Cale> This means that ys will be chosen to be an arbitrary subset of xs
<Cale> (or to be more precise, a sub-list)
<crestfallen> arbitrary? so this is a cartesian product right?
<Cale> Not exactly...
<Cale> Though you can do stuff like:
<Cale> > replicateM 3 [True, False]
<lambdabot>  [[True,True,True],[True,True,False],[True,False,True],[True,False,False],[Fa...
<Cale> This is a Cartesian product
<Cale> > do x <- [1,2,3]; y <- [4,5]; z <- [6,7,8]; return (x,y,z)
<lambdabot>  [(1,4,6),(1,4,7),(1,4,8),(1,5,6),(1,5,7),(1,5,8),(2,4,6),(2,4,7),(2,4,8),(2,...
<Cale> But yeah, in some sense, there's a Cartesian product taking place, and then some other stuff
<crestfallen> but [True,False] is a full inclusion and full exclusion of all combinations of the list
<Cale> It's like we're taking the Cartesian product of [True, False] and (filterMM (\x -> [True, False]) xs)
<Cale> and then for each pair (b, ys)
<Cale> if b is True, we turn that into (x:ys)
<Cale> and if b is False, we produce ys
<crestfallen> so the blogs uses "flg1" but not flg2, is that why I'm thrown off?
<Cale> Maybe?
<Cale> There's no flg2 in that code anyway
<Cale> For some reason, they named one of their variables flg1
<crestfallen> but there are two flags True and False right?
<Cale> It's a completely arbitrary name, feel free to change it
<Cale> flg1 doesn't refer to True or something
<Cale> (well, sometimes it will, sometimes it'll refer to False)
<Cale> all the way
<crestfallen> what would be a better name? I thought each bool in the list would be a flag with a name
<Cale> Well, do you understand what liftA2 does?
<Cale> (in this context)
<crestfallen> essentially returns a cartesian product of list members, I thought
<Cale> Yeah, in some sense
<crestfallen> yeah there's so much going on here, sorry for the confusion
<Cale> liftA2 f xs ys will apply f to each combination of elements from xs and ys
<Cale> i.e.  liftA2 f xs ys = [f x y | x <- xs, y <- ys] -- if you like list comprehensions
<Cale> and the first argument to liftA2 here is (p x)
<Cale> and if p is (\x -> [True, False]), then (p x) is [True, False]
<Cale> So, flg1 will be bound to both True and False at various points, as the liftA2 goes over all the possible combinations
<Cale> This version isn't especially readable
<Cale> Also, "acc" is similarly a bad name, since it gives the impression that something is being accumulated there
<crestfallen> thanks Cale so do you mind showing the Monad version's output, since it's not using TWO accumulators acc and accx  ?
<Cale> But that simply refers to the result of processing the remainder of the list
<Cale> > filterM (\_ -> [True, False]) [1,2,3]
<lambdabot>  [[1,2,3],[1,2],[1,3],[1],[2,3],[2],[3],[]]
<Cale> You'll notice that in the first half of this list, all the lists contain 1, and in the second half, they all don't
<crestfallen> I meant the "trace" of [4,5] just to keep it short
<crestfallen> for the Monad version. oh would scanr work for the applicative version..just thought of that..
<Cale> crestfallen: http://dpaste.com/05223GW
<Cale> crestfallen: I took some liberties with the evaluation, just doing it in whatever order seemed convenient
<Cale> All evaluation orders which terminate are guaranteed to produce the same result in Haskell anyway
<Cale> I also took some liberty when it came to unfolding the (>>=) for the list monad
<Cale> xs >>= f = concat (map f xs)
<crestfallen> one moment thanks so much Cale
<Cale> So, I just did the map by hand quickly, and left the concat
<Cale> There are actually a whole lot more steps that I'm skipping there
<Cale> (but reading through the evaluation of map would be boring)
<crestfallen> I'm using scanr in the FiltM program and it seems like it (also!) skips steps
<Cale> But also, this isn't at all how I think about the meaning of this program
<crestfallen> wow really?
<Cale> It's foolish to unfold recursive functions manually in this way
<crestfallen> that's a drag.. this is how I try to think of programs
<Cale> If you understand what they're meant to do, it's much much easier to simply assume they're going to do what is intended on all smaller inputs
<Cale> and simply try to convince yourself that they do what is required on an input of the given size
<Cale> i.e. you would prove that a recursive function is correct as a proof by induction
<Cale> So it also makes sense to reason about it that way
<Cale> So, when we get to the point of needing to evaluate filterM (\_ -> [True, False]) on the tail of our input list, we can just assume it does the thing we want, and produces a list of all the possible combinations of that tail
<Cale> and then we just need to check that the result will be a list of all the possible combinations of elements of the entire list
<Cale> which it will be, because every combination of (x:xs) will either include x or it won't, and apart from that, will consist of a combination of xs
<Cale> So when we pick b from [True, False], we're saying "either include x or don't"
<Cale> and then we pick ys to be some combination of xs
<Cale> and then we produce the combination x:ys if we meant to keep x, and ys if we didn't mean to keep x
<crestfallen> but the thing is, is that you *know* how to do that (beautiful!) evaluation, so you *do* know what's happening under the hood. I don't but think I may know that I can study your extremely kind solution Cale
<Cale> If you're at the highest level of trusting that things are going to behave as they ought to, filterM (\_ -> [True, False]) [1,2,3] just reads:
<crestfallen> now* that I can study
<Cale> For each element of [1,2,3], take it or leave it (in all possible ways)
<Cale> (regardless of what the value of the list element is)
<crestfallen> yeah I essentially knew that from the textbook, but is there not an advantage to see what haskell is doing?
<Cale> > filterM (\x -> if odd x then [True, False] else [True]) [1,2,3]
<lambdabot>  [[1,2,3],[1,2],[2,3],[2]]
<Cale> ^^ "if x is odd, then take it or leave it, otherwise, be sure to keep it"
<Cale> > filterM (\x -> if odd x then [True] else [False]) [1,2,3] -- this is just a normal filter now
<lambdabot>  [[1,3]]
<Cale> (kind of)
<crestfallen> thanks very kindly Cale for your patience. I understand it on the surface .Can't wait to tear into this. 
<crestfallen> I was thinking about it at night... and tbh the blog evaluation did seem wrong or lacking...
<Cale> Well, even though I gave a fair amount of detail, my evaluation steps are not correctly ordered for how GHC is going to evaluate things, or how a generic lazy evaluator would.
<Cale> and I skipped a lot of steps as well
<Cale> But the nice thing about languages like Haskell where evaluation is pure is that it doesn't matter what order you evaluate things in if all you care about is the result (and you're smart enough not to pick an order which never terminates)
<Cale> Also: If any evaluation order terminates, the order where you evaluate outermost-first will terminate
<crestfallen> pardon?
<Cale> Maybe I should do my example for evaluation orders :)
<Cale> Suppose we have
<Cale> double x = x + x
<Cale> and we want to evaluate  double (double 5)
<crestfallen> k
<Cale> We could evaluate this innermost-first:
<Cale> double (double 5)
<Cale> -> double (5 + 5)
<Cale> -> double 10
<Cale> -> 10 + 10
<Cale> -> 20
<Cale> This is "eager evaluation"
<Cale> (also sometimes "strict evaluation")
<crestfallen> copy that
<Cale> We could also evaluate the outermost double first:
<Cale> double (double 5)
<Cale> -> (double 5) + (double 5)
<Cale> -> (5 + 5) + (double 5)
<Cale> -> 10 + (double 5)
<Cale> -> 10 + (5 + 5)
<Cale> -> 10 + 10
<Cale> -> 20
<Cale> This is sometimes called "normal order evaluation"
<Cale> You might notice a problem with this -- it can be quite wasteful
<Cale> We evaluated double 5 twice
<crestfallen> right
<Cale> because x occurred more than once in the body of the definition of double
<Cale> so we duplicated work
<Cale> So as an optimisation to this, we have lazy evaluation, which is like outermost-first evaluation, except that whenever we bind a variable, we share any work evaluating it between the occurrences
<crestfallen> so is foldr more efficient always or is that a leap?
<Cale> I'm not talking about foldr here
<Cale> But it's not *always* more efficient, just sometimes better
<Cale> So, with lazy evaluation, using "let" syntax to represent the sharing of work:
<Cale> double (double 5)
<Cale> -> let x = double 5 in x + x -- note that we still evaluated the outermost double first
<Cale> -> let x = 5 + 5 in x + x
<Cale> -> let x = 10 in x + x
<Cale> -> 10 + 10
<Cale> -> 20
<Cale> So now we avoided repeating work
<crestfallen> that's good info
<crestfallen> thanks!!
<Cale> and, something not obvious from this example is that outermost-first evaluation can also avoid doing some work altogether
<Cale> In innermost-first evaluation, if the argument to a function goes unused by the function, then you already wasted effort evaluating it
<crestfallen> I find let statements tough, esp when the result is on the lhs
<Cale> oh
<Cale> Is it not clear what is meant there?
<Cale> Really what happens in memory is that there's a graph structure built of pointers
<Cale> and reduction is manipulating these expression graphs
<crestfallen> no I meant in like applicative state, the result of threading state is written on the lhs
<Cale> hm
<Cale> In any case, in innermost first evaluation, we evaluate each of the arguments to the function exactly once
<Cale> In outermost first evaluation, we evaluate each of the arguments to a function zero or more times -- we can skip evaluating them sometimes, but sometimes work is duplicated
<Cale> In lazy evaluation, we evaluate each argument to a function zero or one times as needed.
<crestfallen> like    stf <*> stx = S (\s -> let (f,s') = app stf s (x,s'') = app stx s' in (f x, s''))
<crestfallen> sorry poor sending there
<Cale> I mostly find that hard to read because of the terrible variable names
<crestfallen> thank you yes I thought they were off!!
<Cale> and the fact that runState has been renamed to app
<Cale> errr
<Cale> yeah...
<Cale> and it... doesn't typecheck
<Cale> oh, you left out a ;
<Cale> stf <*> stx = S (\s -> let (f,s') = app stf s; (x,s'') = app stx s' in (f x, s''))
<crestfallen> oh god, I've been on that section of the book for months.   https://termbin.com/eagr   how should it be written?
<Cale> Which book is this?
<crestfallen> hutton
<Cale> ah, hm
<Cale> Okay, so
<Cale> I would usually introduce the State monad as:
<crestfallen> the Applicative instance I'm still struggling with there
<Cale> newtype State s a = S (s -> (s,a))
<Cale> Though often we name the data constructor of the newtype the same as the type constructor (i.e. you can use State for both)
<Cale> It *is* more convenient when learning/teaching to rename the data constructor like this
<Cale> It's possibly confusing to call this ST though, because there is an ST monad, but it's not this
<Cale> and usually the function:
<Cale> runState :: State s a -> s -> (s,a)
<Cale> is called runState
<Cale> Also, there's a choice of convention about whether the pairs are (s,a) or (a,s) -- it obviously doesn't really matter as long as you're consistent :)
<Cale> (but (s,a) is theoretically nicer)
<Cale> I think it's easier to go past the Applicative instance, and just do Monad first, and then never worry about Applicative, because every Monad is an Applicative for free
<crestfallen> yeah thanks you did suggest that once before Cale
